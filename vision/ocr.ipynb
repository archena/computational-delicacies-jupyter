{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optical Character Recognition\n",
    "\n",
    "This notebook explores and compares different methods of optical character recognition Tesseract OCR and Google Vision API. OCR is an important tool, if we want to process large quantities of printed or hand-written textual data. The accompanying blog post can be found [here](https://fuzzylabs.ai/blog/the-battle-of-the-ocr-engines/)\n",
    "\n",
    "The examples provided:\n",
    "* Sample prescription (amoxicillin, a type of antibiotic): `gs://fuzzylabs-jupyter-delicacies/amoxicillin_prescription.png`\n",
    "* Receipt from a hotel (with a hand-written note): `gs://fuzzylabs-jupyter-delicacies/receipt.jpg`\n",
    "\n",
    "You are encouraged to try it on different images (local files and Google Cloud Storage are supported) to experiment and identify which methods and models best suit your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to install the necessary packages, if they are not already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tesserocr matplotlib opencv-python google-cloud-vision gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "from cv2 import cv2\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, 'C')\n",
    "from tesserocr import *\n",
    "from google.cloud import vision\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img, title=\"\"):\n",
    "    plt.figure(figsize=(5,88))\n",
    "    plt.imshow(img)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "# Accepts either a local file \n",
    "def imread_wrapper(uri):\n",
    "    if uri.startswith(\"gs://\"):\n",
    "        with gcsfs.GCSFileSystem().open(uri, \"rb\") as f:\n",
    "            arr = np.asarray(bytearray(f.read()), dtype=\"uint8\")\n",
    "            img = cv2.imdecode(arr, cv2.IMREAD_COLOR)\n",
    "            return img\n",
    "    else: # Assume local file\n",
    "        return cv2.imread(uri, cv2.IMREAD_COLOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the image (filename specified in `img`) to run the recognition on.\n",
    "\n",
    "You can optionally set `PERFORM_THRESHOLDING` to `True`, to perform thresholding. It is advised to preprocess an image if there are unwanted objects in the background (e.g. img/amoxicillin_prescription.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERFORM_THRESHOLDING = False\n",
    "img = imread_wrapper(\"gs://fuzzylabs-jupyter-delicacies/receipt.jpg\")\n",
    "if PERFORM_THRESHOLDING:\n",
    "    _, img = cv2.threshold(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "show_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesseract OCR\n",
    "\n",
    "Tesseract can run in different page segmentation modes. In many cases it is enough to run it on auto (PSM.AUTO), but it is often useful to provide it with some hint on what the layout of the input image is.\n",
    "\n",
    "We consider three modes here:\n",
    "* `PSM.SINGLE_BLOCK` -- Assume a single uniform block of text.\n",
    "* `PSM.SPARSE_TEXT` -- Sparse text. Find as much text as possible in no particular order.\n",
    "* `PSM.SINGLE_COLUMN` -- Assume a single column of text of variable sizes.\n",
    "\n",
    "The illustration of how they behave is provided below.\n",
    "\n",
    "One can read more on different PSMs in Tesseract OCR documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesseract_modes = [\n",
    "    (\"Tesseract SINGLE BLOCK\", PyTessBaseAPI(psm=PSM.SINGLE_BLOCK)),\n",
    "    (\"Tesseract SPARSE TEXT\", PyTessBaseAPI(psm=PSM.SPARSE_TEXT)),\n",
    "    (\"Tesseract SINGLE COLUMN\", PyTessBaseAPI(psm=PSM.SINGLE_COLUMN))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tesseract_ocr(tesseract, img):\n",
    "    pil_image = Image.fromarray(img)\n",
    "    tesseract.SetImage(pil_image)\n",
    "    tesseract.Recognize()\n",
    "    return tesseract.GetIterator()\n",
    "\n",
    "def display_tesseract_results(img, level):\n",
    "    n = len(tesseract_modes)\n",
    "    fig, axs = plt.subplots(1,n, figsize=(5*n, 88))\n",
    "    for (name, tesseract), ax in zip(tesseract_modes, axs):\n",
    "        results = tesseract_ocr(tesseract, img)\n",
    "        _img = img.copy()\n",
    "        tokens = []\n",
    "        for r in iterate_level(results, level):\n",
    "            text = r.GetUTF8Text(level)\n",
    "            tokens += [text]\n",
    "            bb = r.BoundingBox(level)\n",
    "            cv2.rectangle(_img, (bb[0], bb[1]), (bb[2], bb[3]), (255, 0, 0), 2)\n",
    "        print(name)\n",
    "        print(tokens)\n",
    "        print()\n",
    "        ax.set_title(name)\n",
    "        ax.imshow(_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results (i.e. recognised texts and their locations) that are returned by Tesseract are arranged in a hierarchy, and therefore can be accessed at different levels. In this notebook, we show the results by paragraph, text line and word. One may find other levels (namely blocks of text and characters) useful for their application. \n",
    "\n",
    "## By paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tesseract_results(img, RIL.PARA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By text line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tesseract_results(img, RIL.TEXTLINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tesseract_results(img, RIL.WORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Vision API\n",
    "\n",
    "As an alternative to Tesseract OCR, we also explore Google Vision. As opposed to Tesseract, it does not run locally, but on the Google's servers. In order to use it, valid credentials need to be provided in `GOOGLE_APPLICATION_CREDENTIALS` environment variable (more details in the Google's documentation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_image_annotator = vision.ImageAnnotatorClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_document_text_detection(img):\n",
    "    buf = io.BytesIO()\n",
    "    img_pil = Image.fromarray(img)\n",
    "    img_pil.save(buf, 'PNG')\n",
    "    byte_image = buf.getvalue()\n",
    "\n",
    "    document_image = vision.types.Image(content=byte_image)\n",
    "    image_response = google_image_annotator.document_text_detection(image=document_image, image_context={\"language_hints\": [\"en\"]})\n",
    "    page = image_response.full_text_annotation.pages[0]\n",
    "    return page\n",
    "\n",
    "def google_text_detection(img):\n",
    "    buf = io.BytesIO()\n",
    "    img_pil = Image.fromarray(img)\n",
    "    img_pil.save(buf, 'PNG')\n",
    "    byte_image = buf.getvalue()\n",
    "\n",
    "    document_image = vision.types.Image(content=byte_image)\n",
    "    image_response = google_image_annotator.text_detection(image=document_image, image_context={\"language_hints\": [\"en\"]})\n",
    "    page = image_response.full_text_annotation.pages[0]\n",
    "    return page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Vision API does not have options for differents modes of page segmentation, however there's still some degree of flexibility provided. There are two models that are tuned on two different tasks:\n",
    "\n",
    "* Text Detection model -- detects and extracts text from any image\n",
    "* Document Text Detection model -- performs the same operation, but is tuned for dense texts and documents\n",
    "\n",
    "The choice of the model depends on the nature and structure of the images to be recognised (same as with PSM of Tesseract OCR). We show the results produced by both models in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [\n",
    "    (\"Google Text Detection\", google_text_detection(img)),\n",
    "    (\"Google Document Text Detection\", google_document_text_detection(img)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_symbol_text(symbol):\n",
    "    text = symbol.text\n",
    "    if symbol.property.HasField('detected_break'):\n",
    "        if symbol.property.detected_break == vision.enums.TextAnnotation.DetectedBreak.BreakType.LINE_BREAK:\n",
    "            text += \"\\n\"\n",
    "        else:\n",
    "            text += \" \"\n",
    "    return text\n",
    "\n",
    "def get_word_text(word):\n",
    "    return \"\".join([get_symbol_text(x) for x in word.symbols])\n",
    "\n",
    "def get_paragraph_text(para):\n",
    "    return \"\".join([get_word_text(x) for x in para.words])\n",
    "\n",
    "def get_block_text(block):\n",
    "    return \"\".join([get_paragraph_text(x) for x in block.paragraphs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bounding_box(img, bb):\n",
    "    cv2.polylines(img, [np.array([[point.x, point.y] for point in bb])], True, (255, 0, 0), 2)\n",
    "    \n",
    "\n",
    "def get_google_words(page, img):\n",
    "    _img = img.copy()\n",
    "    texts = []\n",
    "    for block in page.blocks:\n",
    "        for para in block.paragraphs:\n",
    "            for word in para.words:\n",
    "                texts += [get_word_text(word)]\n",
    "                add_bounding_box(_img, word.bounding_box.vertices)\n",
    "    return texts, _img\n",
    "    \n",
    "def get_google_paragraphs(page, img):\n",
    "    _img = img.copy()\n",
    "    texts = []\n",
    "    for block in page.blocks:\n",
    "        for para in block.paragraphs:\n",
    "            texts += [get_paragraph_text(para)]\n",
    "            add_bounding_box(_img, para.bounding_box.vertices)\n",
    "    return texts, _img\n",
    "    \n",
    "def get_google_blocks(page, img):\n",
    "    _img = img.copy()\n",
    "    texts = []\n",
    "    for block in page.blocks:\n",
    "        texts += [get_block_text(block)]\n",
    "        add_bounding_box(_img, block.bounding_box.vertices)\n",
    "    return texts, _img\n",
    "    \n",
    "def display_google_results(pages, img, func):\n",
    "    n = len(pages)\n",
    "    fig, axs = plt.subplots(1,n, figsize=(5*n, 88))\n",
    "    for (name, page), ax in zip(pages, axs):\n",
    "        texts, _img = func(page, img)\n",
    "        print(name)\n",
    "        print(texts)\n",
    "        print()\n",
    "        ax.imshow(_img)\n",
    "        ax.set_title(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Tesseract OCR, Google Vision also provides results in a hierarchical manner. Hence, they can be browsed at different levels. As an example, iteration over text blocks, paragraphs and individual words are shown below.\n",
    "\n",
    "## By text block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_google_results(pages, img, get_google_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_google_results(pages, img, get_google_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_google_results(pages, img, get_google_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
